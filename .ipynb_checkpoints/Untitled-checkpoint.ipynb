{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "189ec01d-b212-4d62-b48b-6badad7dfb90",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MnistBGD_LS(object):\n",
    "    \"\"\" Batch Gradient Descent with Learning Schedule\n",
    "    \"\"\"\n",
    "    def __init__(self, n_x, n_h1, n_y, eta = 0.1, epochs = 100, random_seed=1):\n",
    "        \"\"\" \n",
    "        \"\"\"\n",
    "        self.n_x = n_x\n",
    "        self.n_h = n_h\n",
    "        self.n_y = n_y\n",
    "        self.eta = eta\n",
    "        self.epochs = epochs\n",
    "        np.random.seed(random_seed)\n",
    "        self.W1 = 2*np.random.random((self.n_h, self.n_x)) - 1  # between -1 and 1\n",
    "        self.W2 = 2*np.random.random((self.n_y, self.n_h)) - 1  # between -1 and 1\n",
    "        print('W1.shape={}, W2.shape={}'.format(self.W1.shape, self.W2.shape))\n",
    "        \n",
    "    def forpass(self, A0):\n",
    "        Z1 = np.dot(self.W1, A0)        # hidden layer inputs\n",
    "        A1 = self.g(Z1)                 # hidden layer outputs/activation func\n",
    "        Z2 = np.dot(self.W2, A1)        # output layer inputs\n",
    "        A2 = self.g(Z2)                 # output layer outputs/activation func\n",
    "        return Z1, A1, Z2, A2\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        self.cost_ = []\n",
    "        self.m_samples = len(y)       \n",
    "        Y = joy.one_hot_encoding(y, self.n_y)     \n",
    "        # learning rate is scheduled to decrement by a step of \n",
    "        # which the inteveral from self.eta to 0.0001 eqaully \n",
    "        # divided by total number of iterations(epochs or \n",
    "        # epochs * m_samples)\n",
    "        eta_scheduled = np.linspace(self.eta, 0.0001, self.epochs)\n",
    "        \n",
    "        # for momentum\n",
    "        #self.v1 = np.zeros_like(self.W1)\n",
    "        #self.v2 = np.zeros_like(self.W2)\n",
    "        \n",
    "        for epoch in range(self.epochs):\n",
    "            if epoch % 1000 == 0:\n",
    "                print('Training epoch {}/{}.'.format(epoch, self.epochs))\n",
    "\n",
    "            A0 = np.array(X, ndmin=2).T       \n",
    "            Y0 = np.array(Y, ndmin=2).T     \n",
    "\n",
    "            Z1, A1, Z2, A2 = self.forpass(A0)  \n",
    "            E2 = Y0 - A2                      \n",
    "            E1 = np.dot(self.W2.T, E2)         \n",
    "\n",
    "            dZ2 = E2 * self.g_prime(Z2)          \n",
    "            dZ1 = E1 * self.g_prime(Z1)       \n",
    "            \n",
    "            # udpate weight with momentum\n",
    "            #eta = learning_schedule[epoch]\n",
    "            #self.v2 = 0.9 * self.v2 + self.eta * np.dot(dZ2, A1.T) / m_samples\n",
    "            #self.v1 = 0.9 * self.v1 + self.eta * np.dot(dZ1, A0.T) / m_samples\n",
    "            #self.W2 += self.v2     \n",
    "            #self.W1 += self.v1 \n",
    "\n",
    "            # update weights without momentum\n",
    "            # eta = eta_scheduled[epoch]\n",
    "            self.W2 +=  self.eta * np.dot(dZ2, A1.T) / self.m_samples    \n",
    "            self.W1 +=  self.eta * np.dot(dZ1, A0.T) / self.m_samples    \n",
    "            self.cost_.append(np.sqrt(np.sum(E2 * E2)))\n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        A0 = np.array(X, ndmin=2).T         # A0: inputs\n",
    "        Z1, A1, Z2, A2 = self.forpass(A0)   # forpass\n",
    "        return A2                                       \n",
    "\n",
    "    def g(self, x):                 # activation_function: sigmoid\n",
    "        x = np.clip(x, -500, 500)   # prevent from overflow, \n",
    "        return 1.0/(1.0+np.exp(-x)) # stackoverflow.com/questions/23128401/\n",
    "                                    # overflow-error-in-neural-networks-implementation\n",
    "    \n",
    "    def g_prime(self, x):                    # activation_function: sigmoid derivative\n",
    "        return self.g(x) * (1 - self.g(x))\n",
    "    \n",
    "    def evaluate(self, Xtest, ytest):       # fully vectorized calculation\n",
    "        m_samples = len(ytest)\n",
    "        scores = 0        \n",
    "        A2 = self.predict(Xtest)\n",
    "        yhat = np.argmax(A2, axis = 0)\n",
    "        scores += np.sum(yhat == ytest)\n",
    "        return scores/m_samples * 100\n",
    "    \n",
    "    def evaluate_onebyone(self, Xtest, ytest):\n",
    "        m_samples = len(ytest)\n",
    "        scores = 0\n",
    "        for m in range(m_samples):\n",
    "            A2 = nn.predict(Xtest[m])\n",
    "            yhat = np.argmax(A2)\n",
    "            if yhat == ytest[m]:\n",
    "                scores += 1        \n",
    "        return scores/m_samples * 100\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
